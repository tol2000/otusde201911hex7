= Безопасный Бостон

== Краткое описание проекта

В общем и целом, данный проект я делал при помощи движка spark SQL,
поскольку в SQL у меня больше опыта и поскольку я хотел дополнительно
изучить мощь Hive-диалекта SQL в спарке.

Тем не менее, я посчитал, что было бы неправильно обойти мощь типизированного
dataset api и не использовать все те возможности, которые он представляет.

Справочник offense_codes.csv, как я выяснил, содержит дублирующиеся коды,
которые при объединении искажали бы результаты (из-за дублирующихся строк).
Поэтому в начала приложения я создал датасет offenseCodesDs, в котором убрал дубликаты уже
при помощи не-sql варианта dataset api, используя то, что давал на лекции Егор.

Также в конце я сделал контрольный подсчет трех наиболее часто встречающихся преступлений
конкретного района при помощи dataset api для сверки.

=== Описание SQL-движка

По датасетам crime и offense_codes создал три запроса, каждый из которых вычисляет свою подгруппу. +
Один - общие итоги, один - помесячные и один - аналитические.

Затем в общем запросе я объединяю эти три запроса в один результат и записываю в паркет.

[NOTE]
Район null трансформировал в 00 для более понятного джойна и сортировки.

[NOTE]
Хотел как минимум объединить помесячный запрос с общими итогами, чтобы не считать несколько раз,
но наткнулся на неприятную погрешность: среднее от среднемесячных существенно отличалось от
общесреднего (поля lat и lng). +
Поэтому оставил как есть: каждый запрос считает свое.

== Запуск и вывод продакшн-версии

Вывод на экран состоит из нескольких частей:

* Вывод планов запросов (чтобы убедиться, что используется broadcast)
* Вывод основной таблицы и запись ее в паркет
* Вывод контрольной таблицы, посчитанной при помощи типизированного dataset api

Вывод в паркет в каталог, переданный в параметрах, как указано в условии задачи

[WARNING]
Каталог, в который выводится паркет, предварительно полностью очищается, будьте внимательны! +
(это проделки опции overwrite).

== Вывод проекта

[Source, bash]
----
----
